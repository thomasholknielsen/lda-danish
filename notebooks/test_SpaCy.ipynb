{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spacy test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.da import Danish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = Danish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank('da')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('xx_ent_wiki_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xx_ent_wiki_sm as da\n",
    "nlp = da.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ner']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ner', <spacy.pipeline.EntityRecognizer at 0x1b29f92eca8>)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = u'''Hej Thomas. Jeg henlægger sagen vedrørende de forskellige timer og håber du får dem udbetalt snart. Venlig hilsen Dorte Ellekær'''\n",
    "test = '''Til AKA. Som svar på tidligere stillet spørgsmål, så har jeg siden fratrædelse af stilling afholdt ferie. Ferie der ikke tidligere var afholdt på grund af barsel og sygdom. Med venlig hilsen Casper Lassen'''\n",
    "# test = '''Hej Aka. Hvorfor har jeg fået mindre løn i september 2018 end jeg plejer? Mvh. Jalal'''\n",
    "# test = \"\"\"Jeg bekræfter hermed at nedenstående er korrekt: Du bad desuden om at du ikke får dagpenge i december, fordi din arbejdsgiver Eurotech ikke udbetaler dig løn i december 2017, har haft en uoverensstemmels og om din løn og arbejdsgiver ønsker ikke længere at gøre brug af din arbejdskraft.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Til', ''), ('AKA', ''), ('.', ''), ('Som', ''), ('svar', ''), ('på', ''), ('tidligere', ''), ('stillet', ''), ('spørgsmål', ''), (',', ''), ('så', ''), ('har', ''), ('jeg', ''), ('siden', ''), ('fratrædelse', ''), ('af', ''), ('stilling', ''), ('afholdt', ''), ('ferie', ''), ('.', ''), ('Ferie', ''), ('der', ''), ('ikke', ''), ('tidligere', ''), ('var', ''), ('afholdt', ''), ('på', ''), ('grund', ''), ('af', ''), ('barsel', ''), ('og', ''), ('sygdom', ''), ('.', ''), ('Med', ''), ('venlig', ''), ('hilsen', ''), ('Casper', ''), ('Lassen', '')]\n"
     ]
    }
   ],
   "source": [
    "print([(token.text, token.tag_) for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Til', 'NNP'), ('AKA', 'NNP'), ('.', '.'), ('Som', 'NNP'), ('svar', 'VBD'), ('på', 'NN'), ('tidligere', 'RB'), ('stillet', 'JJ'), ('spørgsmål', 'NN'), (',', ','), ('så', 'JJ'), ('har', 'NN'), ('jeg', 'NN'), ('siden', 'NN'), ('fratrædelse', 'JJ'), ('af', 'NN'), ('stilling', 'VBG'), ('afholdt', 'JJ'), ('ferie', 'NN'), ('.', '.'), ('Ferie', 'NNP'), ('der', 'NN'), ('ikke', 'NN'), ('tidligere', 'RB'), ('var', 'JJ'), ('afholdt', 'NN'), ('på', 'NN'), ('grund', 'NN'), ('af', 'JJ'), ('barsel', 'NN'), ('og', 'NN'), ('sygdom', 'NN'), ('.', '.'), ('Med', 'NNP'), ('venlig', 'JJ'), ('hilsen', 'NN'), ('Casper', 'NNP'), ('Lassen', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "print(nltk.pos_tag(nltk.word_tokenize(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jjnn_pairs(phrase):\n",
    "    '''\n",
    "    Iterate over pairs of JJ-NN.\n",
    "    '''\n",
    "    tagged = nltk.pos_tag(nltk.word_tokenize(phrase))\n",
    "    for ngram in ngramise(tagged):\n",
    "#         print(len(ngram))\n",
    "#         if len(ngram)<=2:\n",
    "#             token, tag = ngram\n",
    "#             if tag == ('NNP'):\n",
    "#                 yield ngram\n",
    "#         else:\n",
    "        tokens, tags = zip(*ngram)\n",
    "        if tags == ('JJ', 'NN'):\n",
    "            yield tokens\n",
    "\n",
    "def ngramise(sequence):\n",
    "    '''\n",
    "    Iterate over bigrams and 1,2-skip-grams.\n",
    "    '''\n",
    "#     for unigram in sequence:\n",
    "#         yield unigram\n",
    "    for bigram in nltk.ngrams(sequence, 2):\n",
    "        yield bigram\n",
    "    for trigram in nltk.ngrams(sequence, 3):\n",
    "        yield trigram[0], trigram[2]\n",
    "    for quadgram in nltk.ngrams(sequence, 4):\n",
    "        yield quadgram[0], quadgram[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Til AKA . Som svar på tidligere stillet spørgsmål , så har jeg siden fratrædelse af stilling afholdt ferie . Ferie der ikke tidligere var afholdt på grund af barsel og sygdom . Med venlig hilsen Casper Lassen\n",
      "\n",
      "\n",
      "[('Til', 'NNP'), ('AKA', 'NNP'), ('.', '.'), ('Som', 'NNP'), ('svar', 'VBD'), ('på', 'NN'), ('tidligere', 'RB'), ('stillet', 'JJ'), ('spørgsmål', 'NN'), (',', ','), ('så', 'JJ'), ('har', 'NN'), ('jeg', 'NN'), ('siden', 'NN'), ('fratrædelse', 'JJ'), ('af', 'NN'), ('stilling', 'VBG'), ('afholdt', 'JJ'), ('ferie', 'NN'), ('.', '.'), ('Ferie', 'NNP'), ('der', 'NN'), ('ikke', 'NN'), ('tidligere', 'RB'), ('var', 'JJ'), ('afholdt', 'NN'), ('på', 'NN'), ('grund', 'NN'), ('af', 'JJ'), ('barsel', 'NN'), ('og', 'NN'), ('sygdom', 'NN'), ('.', '.'), ('Med', 'NNP'), ('venlig', 'JJ'), ('hilsen', 'NN'), ('Casper', 'NNP'), ('Lassen', 'NNP')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('stillet', 'spørgsmål'),\n",
       " ('så', 'har'),\n",
       " ('fratrædelse', 'af'),\n",
       " ('afholdt', 'ferie'),\n",
       " ('var', 'afholdt'),\n",
       " ('af', 'barsel'),\n",
       " ('venlig', 'hilsen'),\n",
       " ('så', 'jeg'),\n",
       " ('var', 'på'),\n",
       " ('af', 'og'),\n",
       " ('så', 'jeg'),\n",
       " ('var', 'på'),\n",
       " ('af', 'og')]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(string)\n",
    "print('\\n')\n",
    "print(nltk.pos_tag(nltk.word_tokenize(test)))\n",
    "list(jjnn_pairs(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"text\\tidx\\tlemma\\tpunct\\tspace\\tshape\\tpos\\ttag\\tdep\\taplha\\tis_stop\".expandtabs(12))\n",
    "for token in doc:\n",
    "    print(\"{0}\\t{1}\\t{2}\\t{3}\\t{4}\\t{5}\\t{6}\\t{7}\\t{8}\\t{9}\\t{10}\".format(\n",
    "        token.text,\n",
    "        token.idx,\n",
    "        token.lemma_,\n",
    "        token.is_punct,\n",
    "        token.is_space,\n",
    "        token.shape_,\n",
    "        token.pos_,\n",
    "        token.tag_, \n",
    "        token.dep_, \n",
    "        token.is_alpha, \n",
    "        token.is_stop\n",
    "    ).expandtabs(12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [x.text for x in doc] tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for token in doc:\n",
    "#     print(token, token.lemma, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = ''\n",
    "for token in doc:\n",
    "    string = string +' '+ token.lemma_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ent in doc.ents:\n",
    "#     print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for chunk in doc.noun_chunks:\n",
    "#     print(chunk.text, chunk.root.text, chunk.root.dep_, chunk.root.head.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from spacy.lemmatizer import Lemmatizer\n",
    "# from spacy.lang.en import LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES\n",
    "\n",
    "# lemmatizer = Lemmatizer(LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES)\n",
    "# lemmas = lemmatizer(u'ducks', u'NOUN')\n",
    "# print(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Lemmy\n",
    "# https://github.com/sorenlind/lemmy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lemmy.pipe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E090] Extension 'lemma' already exists on Token. To overwrite the existing extension, set `force=True` on `Token.set_extension`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-eaeec1effefc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# create an instance of Lemmy's pipeline component for spaCy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpipe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlemmy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\tools\\Anaconda3\\envs\\aka-diagnostic\\lib\\site-packages\\lemmy\\pipe\\component.py\u001b[0m in \u001b[0;36mload\u001b[1;34m()\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mLemmyPipelineComponent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdefault_rules\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\tools\\Anaconda3\\envs\\aka-diagnostic\\lib\\site-packages\\lemmy\\pipe\\component.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, rules)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;31m# Add attributes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0mToken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_extension\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lemma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mtoken.pyx\u001b[0m in \u001b[0;36mspacy.tokens.token.Token.set_extension\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: [E090] Extension 'lemma' already exists on Token. To overwrite the existing extension, set `force=True` on `Token.set_extension`."
     ]
    }
   ],
   "source": [
    "# create an instance of Lemmy's pipeline component for spaCy\n",
    "pipe = lemmy.pipe.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the comonent to the spaCy pipeline.\n",
    "nlp.add_pipe(pipe, after='ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmas can now be accessed using the `._.lemma` attribute on the tokens\n",
    "doc = nlp(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmas can now be accessed using the `._.lemma` attribute on the tokens\n",
    "nlp(test)[7]._.lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Til AKA . Som svar på tidligere stillet spørgsmål , så har jeg siden fratrædelse af stilling afholdt ferie . Ferie der ikke tidligere var afholdt på grund af barsel og sygdom . Med venlig hilsen Casper Lassen'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = ''\n",
    "for token in doc:\n",
    "    string = string +' '+ token.lemma_# + [0]._.lemma\n",
    "string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Til AKA . Som svar på tidligere stillet spørgsmål , så har jeg siden fratrædelse af stilling afholdt ferie . Ferie der ikke tidligere var afholdt på grund af barsel og sygdom . Med venlig hilsen Casper Lassen'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = ''\n",
    "for token in doc:\n",
    "    string = string +' '+ token.lemma_\n",
    "string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _print_example(text):\n",
    "    row_format = \"{token:12}| {pos:12}| {lemma:12}\"\n",
    "    print(row_format.format(token=\"TOKEN\", pos=\"POS\", lemma=\"LEMMA\"))\n",
    "    print(\"-\"*36)\n",
    "    rows = [(t.orth_, t.pos_, t._.lemma if t._.lemma else \"None\") for t in nlp(text)]\n",
    "    for token, pos, lemma in rows:\n",
    "        print(row_format.format(token=token, pos=pos, lemma=lemma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKEN       | POS         | LEMMA       \n",
      "------------------------------------\n",
      "afholdt     |             | None        \n"
     ]
    }
   ],
   "source": [
    "_print_example(\"afholdt \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:aka-diagnostic]",
   "language": "python",
   "name": "conda-env-aka-diagnostic-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
