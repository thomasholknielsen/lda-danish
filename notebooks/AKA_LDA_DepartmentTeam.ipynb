{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modul import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\akademikernes_diagnostic\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# Common\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyodbc\n",
    "\n",
    "# Text processing\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "\n",
    "# Visualization modules\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Model modules\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer#, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.cluster import KMeans\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.corpora import MmCorpus\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "\n",
    "from gensim.models import phrases, word2vec\n",
    "\n",
    "# Remove unnecessary warnings\n",
    "pd.options.mode.chained_assignment = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lipht_lda import df_lda_preprocessing, lda_preprocess_string, df_lda_features, get_lda_topics, lda_predict_df, lda_predict_string, document_to_bow, df_lda_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ThreadID</th>\n",
       "      <th>ThreadSubject</th>\n",
       "      <th>FirstMessage</th>\n",
       "      <th>FirstMemberMessage</th>\n",
       "      <th>ThreadInitiatedBy</th>\n",
       "      <th>ThreadClass</th>\n",
       "      <th>InDiagnosticScope</th>\n",
       "      <th>ThreadMessageID</th>\n",
       "      <th>ThreadResponsibleDepartment</th>\n",
       "      <th>ThreadResponsibleDepartmentTeam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [ThreadID, ThreadSubject, FirstMessage, FirstMemberMessage, ThreadInitiatedBy, ThreadClass, InDiagnosticScope, ThreadMessageID, ThreadResponsibleDepartment, ThreadResponsibleDepartmentTeam]\n",
       "Index: []"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyodbc\n",
    "server = \"LIPHT-VM-01\"#\"LI-PH-01\"\n",
    "db = \"Akademikernes_MSCRM_Addition\"\n",
    "con = pyodbc.connect('DRIVER={SQL Server};SERVER=' + server + ';DATABASE=' + db)\n",
    "\n",
    "query=\"\"\"\n",
    "SELECT *\n",
    "  FROM [Akademikernes_MSCRM_Addition].[out].[LDA_Messages_persisted]\n",
    "  \"\"\"\n",
    "df_raw = pd.read_sql(query, con)\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "to_pickle() missing 1 required positional argument: 'path'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-8de0dcc6175e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_raw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/LDA_Messages_persisted.pkl'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf_import\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_raw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: to_pickle() missing 1 required positional argument: 'path'"
     ]
    }
   ],
   "source": [
    "df_raw = pd.to_pickle('data/LDA_Messages_persisted.pkl')\n",
    "df_import = df_raw.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_import = pd.read_pickle('data/AKA_rawdata_df_lda_preprocessed.pkl')\n",
    "# df_import = pd.read_pickle('data/AKA_rawdata_with_language.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup Analysis Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'df_B'\n",
    "n_gram = 2\n",
    "sample_size= 10000\n",
    "no_words= 5000\n",
    "no_below= 20 # filter out tokens that appear in less than 15 documents\n",
    "random_state=1\n",
    "research_scope = 'Udbetaling'\n",
    "num_topics = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset for method B\n",
    "# Process 1st MemberMessage, then\n",
    "# Concatenate with Subject\n",
    "df_B = df_import.copy(deep=True)\n",
    "df_B.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_B.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# df_B = df_B[(df_B['ThreadStatus']=='Fuldført') & (df_B['ThreadMessageDirection']=='Indgående') & (df_B['ThreadMessageIsFirstMemberMessage']==1) & (df_B['ThreadTotalMessageCount']>1) & (df_B['ThreadHasInteraction']>=1) & (df_B['ThreadResponsibleDepartmentTeam']=='Udbetalingsteam') & (df_B['pred_label']=='Danish')]\n",
    "df_B = df_B[(df_B['ThreadStatus']=='Fuldført') & (df_B['ThreadMessageDirection']=='Indgående') & (df_B['ThreadMessageIsFirstMemberMessage']==1) & (df_B['ThreadResponsibleDepartmentTeam'].str.contains('Udbetalingsteam')==True) & (df_B['pred_label']=='Danish')]\n",
    "print(df_B.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_B.ThreadResponsibleDepartmentTeam.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_lda_preprocessing(df_B,'ThreadMessageText',n_gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_B.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_B.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_B.to_pickle('data/df_B.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Requests\n",
    "Messages are either\n",
    "- Incoming (from members) or\n",
    "- Outgoing (from aka)\n",
    "All messeages have\n",
    "- subject_field and\n",
    "- message_field\n",
    "\n",
    "In the following we will analyze the different splits of data, with regards to the above:\n",
    "- Incoming_subject\n",
    "- Incoming_message\n",
    "- Outgoing_subject\n",
    "- Outgoing_message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Request Analysis: DepartmentTeam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Load the data, and print rows, columns\n",
    "df_scope = pd.read_pickle('data/{}.pkl'.format(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scope.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_scope_name = research_scope +'_topics-'+ str(num_topics) +'_Sample-'+str(sample_size) +'_WordCount-'+str(no_words) +'_RandomState-'+str(random_state)+'_dataset-'+ dataset\n",
    "print(data_scope_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create dictionary and top words\n",
    "\n",
    "##### Parameters:\t\n",
    "- **no_below** (int, optional) – Keep tokens which are contained in at least no_below documents.\n",
    "- **no_above** (float, optional) – Keep tokens which are contained in no more than no_above documents (fraction of total corpus size, not an absolute number).\n",
    "- **keep_n** (int, optional) – Keep only the first keep_n most frequent tokens.\n",
    "- **keep_tokens** (iterable of str) – Iterable of tokens that must stay in dictionary after filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Create dictionary with words from df_scope (the total dataset)\n",
    "dictionary = Dictionary(documents=df_scope.stemmed_text.values)\n",
    "print(\"Found {} words.\".format(len(dictionary.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# dictionary.filter_extremes(no_above=0.8, no_below=3)\n",
    "dictionary.filter_extremes(no_below=no_below, keep_n=no_words)\n",
    "\n",
    "dictionary.compactify()  # Reindexes the remaining words after filtering\n",
    "print(\"Left with {} words.\".format(len(dictionary.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Make a BoW for every Besked\n",
    "document_to_bow(df_scope, dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Sample of Scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scope_lda_sample = df_scope.sample(sample_size, random_state=random_state)\n",
    "scope_lda_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scope_lda_sample.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scope_lda_sample[['ThreadID','ThreadMessageID','ThreadSubject','ThreadMessageText','text','tokenized_text','stopwords_removed','lemmatized_text','stemmed_text']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scope_lda_sample['text'][592345]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find optimal number of topics for LDA\n",
    "#### K-means Elbow method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Topwords\n",
    "Create a list of topwords from the entire dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tf-idf and document similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I define term frequency-inverse document frequency (tf-idf) vectorizer parameters and then convert the clean_content list into a tf-idf matrix.\n",
    "\n",
    "To get a Tf-idf matrix, first count word occurrences by request. This is transformed into a request-term matrix (dtm). This is also just called a term frequency matrix.\n",
    "\n",
    "Then apply the term frequency-inverse document frequency weighting: words that occur frequently within a request but not frequently within the corpus receive a higher weighting as these words are assumed to contain more meaning in relation to the request.\n",
    "\n",
    "A couple things to note about the parameters I define below:\n",
    "\n",
    "max_df: this is the maximum frequency within the request a given feature can have to be used in the tfi-idf matrix. If the term is in greater than 80% of the request it probably cares little meanining - rule of thumb (verify this)\n",
    "\n",
    "min_idf: this could be an integer (e.g. 5) and the term would have to be in at least 5 of the request to be considered. Here I pass 0.2; the term must be in at least 20% of the request. \n",
    "\n",
    "TEST THIS\n",
    "I found that if I allowed a lower min_df I ended up basing clustering on names--for example \"Michael\" or \"Tom\" are names found in several of the movies and the synopses use these names frequently, but the names carry no real meaning.\n",
    "\n",
    "ngram_range: this just means I'll look at unigrams, bigrams and trigrams. See n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "top_words = [v for v in dictionary.values()]\n",
    "top_words = list(set(top_words))\n",
    "df_scope['OnlyTopWords'] = list(map(lambda doc: [word for word in doc if word in top_words], df_scope['stemmed_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"No of top words: {} \".format(len(top_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "top_words, _ = remove_not_topwords(scope_lda_sample, df_scope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create dictionary with words from df_scope (the total dataset) or scope_lda_sample (the sample size)\n",
    "# dictionary = Dictionary(documents=df_scope.stemmed_text.values)\n",
    "# #Make a BoW for every Besked\n",
    "# document_to_bow(df_scope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA preprocessing\n",
    "print(\"Found {} words.\".format(len(dictionary.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "scope_lda_sample['clean_content'] = scope_lda_sample['OnlyTopWords'].apply(ListToString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tfidf_wordvector = TfidfVectorizer(\n",
    "                analyzer='word', \n",
    "                max_df=0.8, \n",
    "                min_df=5, \n",
    "#                 stop_words=stopwords.words('danish'),\n",
    "#                 ngram_range=(1,3)\n",
    "                ) \n",
    "\n",
    "#fit the tfidf_wordvector to clean_content\n",
    "tfidf_wordvector_maxtrix = tfidf_wordvector.fit_transform(scope_lda_sample.clean_content)\n",
    "print(tfidf_wordvector_maxtrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dist is defined as 1 - the cosine similarity of each request. Cosine similarity is measured against the tf-idf matrix and can be used to generate a measure of similarity between each request and the other request in the corpus (each clean_content among the total clean_content). Subtracting it from 1 provides cosine distance which I will use for plotting on a euclidean (2-dimensional) plane.\n",
    "\n",
    "Note that with dist it is possible to evaluate the similarity of any two or more clean_content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_wordvector_2d = tfidf_wordvector_maxtrix.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_range = 151\n",
    "increments = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "distortions = []\n",
    "K = range(1,top_range,increments)\n",
    "for k in K:\n",
    "    kmeanModel = KMeans(n_clusters=k, n_jobs=-1, random_state=0).fit(tfidf_wordvector_2d)\n",
    "    kmeanModel.fit(tfidf_wordvector_2d)\n",
    "    distortions.append(sum(np.min(cdist(tfidf_wordvector_2d, kmeanModel.cluster_centers_, 'euclidean'), axis=1)) / tfidf_wordvector_2d.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the elbow\n",
    "plt.figure(figsize=(16, 10))\n",
    "plt.plot(K, distortions, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('The Elbow Method: {4}. Showing the optimal k\\nSample Size: {0}, Top {1} Words, with increments of {2} from 0 to {3}'.format(sample_size, len(top_words), increments, top_range-1, data_scope_name))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Model Training\n",
    "Latent Dirichlet Allocation (LDA) is generative approach in classifying texts. It is a three level hierarchical Bayesian model where it creates probabilities on word level, on document level and on corpus level (corpus means all documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to maximize the probability of the corpus in the training set.\n",
    "corpus = scope_lda_sample.bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(('LDA Model based on {3} dataset.\\n\\tSample Size: {0},\\n\\tTop {1} Words,\\n\\tNo of Topics {2}'.format(sample_size, len(dictionary.values()), num_topics, data_scope_name)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Input num_topics from the analysis above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#A multicore approach to decrease training time\n",
    "# https://radimrehurek.com/gensim/corpora/mmcorpus.html\n",
    "# ram_corpus = get_tmpfile(\"corpus_scope.mm\")\n",
    "# MmCorpus.serialize(ram_corpus, corpus)\n",
    "# mm = MmCorpus(ram_corpus)\n",
    "LDAmodel_scope = LdaMulticore(corpus=corpus,#mm,\n",
    "                        id2word=dictionary,\n",
    "                        num_topics=num_topics,\n",
    "                        workers=4,\n",
    "                        chunksize=5000,\n",
    "                        passes=50,\n",
    "                        alpha='asymmetric',\n",
    "                        random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.save('data/model/{0}_LDAmodel_dictionary.pkl'.format(data_scope_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDAmodel_scope.save('data/model/{0}_LDAmodel'.format(data_scope_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDAmodel_scope = LdaMulticore.load('data/model/{0}_LDAmodel'.format(data_scope_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_lda_features(LDAmodel_scope, scope_lda_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic distributions and let's see some words that come with the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RequestTopicDistribution = scope_lda_sample['lda_features'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(1,1,figsize=(20,6))\n",
    "nr_top_bars = 3\n",
    "ax1.set_title(\"Request Topic distributions\", fontsize=16)\n",
    "\n",
    "for ax, distribution, color in zip([ax1], [RequestTopicDistribution], ['r']):\n",
    "    # Individual distribution barplots\n",
    "    ax.bar(range(len(distribution)), distribution, alpha=0.7)\n",
    "    rects = ax.patches\n",
    "    for i in np.argsort(distribution)[-nr_top_bars:]:\n",
    "        rects[i].set_color(color)\n",
    "        rects[i].set_alpha(1)\n",
    "\n",
    "fig.tight_layout(h_pad=3.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect topics and words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lipht_lda import get_topics_and_probability, get_lda_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_topics_and_probability(scope_lda_sample, LDAmodel_scope, num_topics, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_lda_topics(scope_lda_sample, LDAmodel_scope, num_topics,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Name the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_topic_names = {\n",
    "    0:'Ferie og feriepenge',\n",
    "    1:'Sendt oplysninger til AKA',\n",
    "    2:'Ansættelseskontrakt eller frigørelse',\n",
    "    3:'Spørgsmål om dagpenge',\n",
    "    4:'Ansøgning om befordring',\n",
    "    5:'Ansættelse',\n",
    "    6:'Ledighed',\n",
    "    7:'Adgang',\n",
    "    8:'Noget med tid*',\n",
    "    9:'Dagpenge mellem jul og nytår',\n",
    "    11:'Fejl ved dagpenge',\n",
    "    12:'Spørgsmål til blanket',\n",
    "    14:'Ydelseskort',\n",
    "    15:'Pension og Efterløn',\n",
    "    16:'Dagpenge/Supplerende',\n",
    "    17:'Spørgsmål til udfyldelse',\n",
    "    19:'Spørgsmål om beskæftigelse'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# document = scope_lda_sample.sample(1) # From sample\n",
    "document = df_scope.sample(1) # From population\n",
    "doc_id = document['ThreadMessageID']\n",
    "unseen_document = document['ThreadMessageText']\n",
    "print(doc_id, unseen_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function and prediction\n",
    "print(lda_predict_string(unseen_document, LDAmodel_scope, dictionary,lda_topic_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vector = dictionary.doc2bow(lda_preprocess_string(unseen_document))\n",
    "for index, score in sorted(LDAmodel_scope[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, LDAmodel_scope.print_topic(index, 5)))\n",
    "\n",
    "# index, score = sorted(LDAmodel_scope[bow_vector], key=lambda tup: -1*tup[1])[0]\n",
    "# print(\"Score: {}\\t Topic: {}\".format(score, LDAmodel_scope.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict topics on data\n",
    "Per every row in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lda_predict_df(df_scope, LDAmodel_scope, dictionary, lda_topic_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the data with prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scope.to_pickle('data/AKA_{0}_with_prediction.pkl'.format(data_scope))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load to MS SQL server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_raw = pd.read_pickle('data/AKA_rawdata.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lipht_lda import df_lda_preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_lda_preprocessing(df_raw, 'ThreadMessageText')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.to_pickle('data/AKA_rawdata_df_lda_preprocessed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_raw[df_raw['pred_label']=='English']['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw_with_language = df_raw[['ThreadID','ThreadMessageID','ThreadMessageText','text','pred_label','pred_probability']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw_with_language.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw_with_language.to_csv('lang_pred.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = \"mssql+pyodbc:///?odbc_connect={}\".format(urllib.parse.quote_plus(\"DRIVER=ODBC Driver 13 for SQL Server;SERVER={0};PORT=1433;DATABASE={1};UID={2};PWD={3};TDS_Version=8.0;\".format(server, db, user, password)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.DataFrame({'test':[1,2,3]}) #'te','te','te'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "from sqlalchemy import create_engine\n",
    "import urllib\n",
    "\n",
    "params = urllib.parse.quote_plus(r'DRIVER={SQL Server};SERVER=LIPHT-VM-01;DATABASE=Akademikernes_MSCRM_addition;Trusted_Connection=yes')\n",
    "conn_str = 'mssql+pyodbc:///?odbc_connect={}'.format(params)\n",
    "engine = create_engine(conn_str)\n",
    "\n",
    "\n",
    "test.to_sql(name='Test',con=engine , schema='input', if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.to_pickle('data/AKA_rawdata_with_language.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:akademikernes_diagnostic]",
   "language": "python",
   "name": "conda-env-akademikernes_diagnostic-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
