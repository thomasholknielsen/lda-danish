{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('tagsets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cur_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dk_pos_tag_dataset = [line.rstrip('\\n') for line in open(os.path.join(cur_dir,'data','pos','RO2012.opslagsord.med.homnr.og.ordklasse.txt'))] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = {}\n",
    "for line in open(os.path.join(cur_dir,'data','pos','RO2012.opslagsord.med.homnr.og.ordklasse.txt')):\n",
    "    line = line.rstrip('\\.\\n').split(';')\n",
    "    test[line[0]] = line[1].upper()\n",
    "danish_pos = test             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ADJ'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "danish_pos['behagelig']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hej', 'Greeting')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BackoffTagger:\n",
    "    def __init__(self):\n",
    "        self._taggers = [nltk.PerceptronTagger()]\n",
    "\n",
    "model = {'hej': 'Greeting', 'example_two': 'NN'}\n",
    "tagger = nltk.tag.UnigramTagger(model=model, backoff=BackoffTagger())\n",
    "tagger.tag([l.lower() for l in ['hej']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spacy test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.da import Danish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = Danish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank('da')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('xx_ent_wiki_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xx_ent_wiki_sm as da\n",
    "nlp = da.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ner']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ner', <spacy.pipeline.EntityRecognizer at 0x26c1ba7bb48>)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hej jeff,\\xa0\\nførst og fremmest tak for en behagelig rådgivning.\\xa0\\njeg har en høflig forspørgsel, som vil lette mit administrative og omkostnings niveau gevaldigt.\\xa0\\n\\njeg har som sagt vækst eventyr ivs, som ingen aktivitet har overhovedet. hvilket jeg kan bevise med bank udtog. har prøvet at se om jeg kan finde ud af tingene i dag, men puha en administrativ jungel.\\xa0\\n\\ndet vil være meget nemmere blot at ændre branche koden i vækst eventyr ivs og navnet. i stedte for at jeg skal bruge tid på betalingserklæring og bruge skats tid på at de skal give mig grønt lys på at jeg ikke skylder dem noget. derudover spare jeg også eventuelle omk. til revisoren. ved lukning skal min bankkonto også lukkes. en erhvervskonto er sæjldent gratis, så i stedet for at lukke en konto der fungere for at åbne en ny, så vil det være nemmere at bruge den eksisterende. jeg vil heller ikke skulle betale gebyr for at få oprettet et nyt ivs ved blot at ændre navn og branche kode. det vil egentlig også spare mig en masse tanker om det hele nu også går igennem som det skal.\\xa0\\n\\nkan du følge min logik? håber du kan hjælpe :-)\\xa0\\n\\nmvh simon\\xa0'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test = u'''Hej Thomas.\\n Jeg henlægger sagen vedrørende de forskellige timer og håber du får dem udbetalt snart. Venlig hilsen Dorte Ellekær'''\n",
    "# test = '''Til AKA. Som svar på tidligere stillet spørgsmål, så har jeg siden fratrædelse af stilling afholdt ferie. Ferie der ikke tidligere var afholdt på grund af barsel og sygdom. Med venlig hilsen Casper Lassen'''\n",
    "test = '''Hej Aka. Hvorfor har jeg fået mindre løn i september 2018 end jeg plejer?\\n Med venlig hilsen Jalal'''\n",
    "# test = \"\"\"Jeg bekræfter hermed at nedenstående er korrekt: Du bad desuden om at du ikke får dagpenge i december, fordi din arbejdsgiver Eurotech ikke udbetaler dig løn i december 2017, har haft en uoverensstemmels og om din løn og arbejdsgiver ønsker ikke længere at gøre brug af din arbejdskraft.\"\"\"\n",
    "test = \"\"\"hej jeff,\\xa0\\nførst og fremmest tak for en behagelig rådgivning.\\xa0\\njeg har en høflig forspørgsel, som vil lette mit administrative og omkostnings niveau gevaldigt.\\xa0\\n\\njeg har som sagt vækst eventyr ivs, som ingen aktivitet har overhovedet. hvilket jeg kan bevise med bank udtog. har prøvet at se om jeg kan finde ud af tingene i dag, men puha en administrativ jungel.\\xa0\\n\\ndet vil være meget nemmere blot at ændre branche koden i vækst eventyr ivs og navnet. i stedte for at jeg skal bruge tid på betalingserklæring og bruge skats tid på at de skal give mig grønt lys på at jeg ikke skylder dem noget. derudover spare jeg også eventuelle omk. til revisoren. ved lukning skal min bankkonto også lukkes. en erhvervskonto er sæjldent gratis, så i stedet for at lukke en konto der fungere for at åbne en ny, så vil det være nemmere at bruge den eksisterende. jeg vil heller ikke skulle betale gebyr for at få oprettet et nyt ivs ved blot at ændre navn og branche kode. det vil egentlig også spare mig en masse tanker om det hele nu også går igennem som det skal.\\xa0\\n\\nkan du følge min logik? håber du kan hjælpe :-)\\xa0\\n\\nmvh simon\\xa0\"\"\"\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hej jeff,',\n",
       " 'først og fremmest tak for en behagelig rådgivning',\n",
       " 'jeg har en høflig forspørgsel, som vil lette mit administrative og omkostnings niveau gevaldigt',\n",
       " 'jeg har som sagt vækst eventyr ivs, som ingen aktivitet har overhovedet',\n",
       " 'hvilket jeg kan bevise med bank udtog',\n",
       " 'har prøvet at se om jeg kan finde ud af tingene i dag, men puha en administrativ jungel',\n",
       " 'det vil være meget nemmere blot at ændre branche koden i vækst eventyr ivs og navnet',\n",
       " 'i stedte for at jeg skal bruge tid på betalingserklæring og bruge skats tid på at de skal give mig grønt lys på at jeg ikke skylder dem noget',\n",
       " 'derudover spare jeg også eventuelle omk',\n",
       " 'til revisoren',\n",
       " 'ved lukning skal min bankkonto også lukkes',\n",
       " 'en erhvervskonto er sæjldent gratis, så i stedet for at lukke en konto der fungere for at åbne en ny, så vil det være nemmere at bruge den eksisterende',\n",
       " 'jeg vil heller ikke skulle betale gebyr for at få oprettet et nyt ivs ved blot at ændre navn og branche kode',\n",
       " 'det vil egentlig også spare mig en masse tanker om det hele nu også går igennem som det skal',\n",
       " 'kan du følge min logik?',\n",
       " 'håber du kan hjælpe :-)',\n",
       " 'mvh simon']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def blobtotext(blob):\n",
    "    def insert (source_str, insert_str, pos):\n",
    "        return source_str[:pos]+insert_str+source_str[pos:]\n",
    "    \n",
    "    blob = blob.replace('\\xa0', ' ')\n",
    "    blob_char_index = [m.end(0) for m in re.finditer(r'[\\?!]', blob)] #re.findall('[\\?.!]', blob)\n",
    "    for i in blob_char_index:\n",
    "        blob = insert(blob,'\\n',i+1)\n",
    "    \n",
    "    list_sent = [s.strip() for s in re.split(r'(\\n)', blob)]\n",
    "    list_sent = list(filter(None,list_sent))\n",
    "    list_sent = [s.strip('.') for s in list_sent]\n",
    "    new = [i[-1] if i[-1]=='.' else i for i in list_sent]\n",
    "\n",
    "    final = []\n",
    "    for i in list_sent:\n",
    "        temp = re.split(r'\\. ',i)\n",
    "        for i in temp:\n",
    "            final.append(i.strip())\n",
    "\n",
    "    return final\n",
    "\n",
    "list_sent = blobtotext(test)\n",
    "list_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def tag_sentences(list_sentences):\n",
    "    def find_matches(item):\n",
    "        tags = {\n",
    "            '.*\\?': 'QUESTION',\n",
    "            '(til|kære|hej|hejsa|hello|hi|dear)': 'GREETING',\n",
    "            '.*(bekræfter.*.hermed.*.korrekt)': 'CONFIRMATION',\n",
    "            '.*(tak)': 'POSITIVE FEEDBACK',\n",
    "            '.*(håber.*.hjælpe)': 'REQUEST HELP',\n",
    "            '.*(vedhæft*)': 'ATTACHMENT',\n",
    "            '(med\\svenlig\\shilsen|de\\sbedste\\shilsner|dbh|venlig\\shilsen|mvh|hilsen|venlig|best\\sregards|with\\skind\\sregards|kind\\sregards|with\\sregards|regards|kh|sent\\sfrom\\smy\\s).*': 'ENDING',\n",
    "#             r'.*(ak\\s?+[0-9]{0,3})': 'AK FORM'\n",
    "            }\n",
    "        for k in tags:\n",
    "            if re.match(k, item):\n",
    "                return tags[k]\n",
    "\n",
    "    matches = []\n",
    "#     for sentence in list_sentences:\n",
    "#         for word in sentence.split():\n",
    "#             if find_matches(word) is not None:\n",
    "#                 matches.append((sentence, find_matches(word)))\n",
    "#                 next\n",
    "    length_sent = []\n",
    "    for sentence in list_sentences:\n",
    "#         length_sent.append(len(sentence))\n",
    "        if find_matches(sentence.lower()) is None:\n",
    "            matches.append((sentence,'CONTENT'))\n",
    "        else:\n",
    "            matches.append((sentence, find_matches(sentence.lower())))\n",
    "            next\n",
    "                \n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hej jeff,', 'GREETING'),\n",
       " ('først og fremmest tak for en behagelig rådgivning', 'POSITIVE FEEDBACK'),\n",
       " ('jeg har en høflig forspørgsel, som vil lette mit administrative og omkostnings niveau gevaldigt',\n",
       "  'CONTENT'),\n",
       " ('jeg har som sagt vækst eventyr ivs, som ingen aktivitet har overhovedet',\n",
       "  'CONTENT'),\n",
       " ('hvilket jeg kan bevise med bank udtog', 'CONTENT'),\n",
       " ('har prøvet at se om jeg kan finde ud af tingene i dag, men puha en administrativ jungel',\n",
       "  'CONTENT'),\n",
       " ('det vil være meget nemmere blot at ændre branche koden i vækst eventyr ivs og navnet',\n",
       "  'CONTENT'),\n",
       " ('i stedte for at jeg skal bruge tid på betalingserklæring og bruge skats tid på at de skal give mig grønt lys på at jeg ikke skylder dem noget',\n",
       "  'CONTENT'),\n",
       " ('derudover spare jeg også eventuelle omk', 'CONTENT'),\n",
       " ('til revisoren', 'GREETING'),\n",
       " ('ved lukning skal min bankkonto også lukkes', 'CONTENT'),\n",
       " ('en erhvervskonto er sæjldent gratis, så i stedet for at lukke en konto der fungere for at åbne en ny, så vil det være nemmere at bruge den eksisterende',\n",
       "  'CONTENT'),\n",
       " ('jeg vil heller ikke skulle betale gebyr for at få oprettet et nyt ivs ved blot at ændre navn og branche kode',\n",
       "  'CONTENT'),\n",
       " ('det vil egentlig også spare mig en masse tanker om det hele nu også går igennem som det skal',\n",
       "  'CONTENT'),\n",
       " ('kan du følge min logik?', 'QUESTION'),\n",
       " ('håber du kan hjælpe :-)', 'REQUEST HELP'),\n",
       " ('mvh simon', 'ENDING')]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_sentences(list_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "LOOKUPS  = [\n",
    "    ('a.*', 'a'),\n",
    "    ('b.*', 'b'),\n",
    "]\n",
    "\n",
    "def lookup(s, lookups):\n",
    "    for pattern, value in lookups:\n",
    "        if re.search(pattern, s):\n",
    "            return value, s\n",
    "    return None\n",
    "\n",
    "print(lookup(\"apple\", LOOKUPS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t= ['først og fremmest tak for en behagelig rådgivning.']\n",
    "for i in t:\n",
    "    if i[-1] == '.':\n",
    "        print(i[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.pos_tag(nltk.word_tokenize(test), lang='dan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = nltk.word_tokenize(test)\n",
    "l\n",
    "# tagger.tag([l.lower() for l in ['Hej','med','dig']])\n",
    "[l.lower() for l in ['Hej','med','dig']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([(token.text, token.tag_) for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nltk.pos_tag(nltk.word_tokenize(test), lang='da'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jjnn_pairs(phrase):\n",
    "    '''\n",
    "    Iterate over pairs of JJ-NN.\n",
    "    '''\n",
    "    tagged = nltk.pos_tag(nltk.word_tokenize(phrase))\n",
    "    for ngram in ngramise(tagged):\n",
    "#         print(len(ngram))\n",
    "#         if len(ngram)<=2:\n",
    "#             token, tag = ngram\n",
    "#             if tag == ('NNP'):\n",
    "#                 yield ngram\n",
    "#         else:\n",
    "        tokens, tags = zip(*ngram)\n",
    "        if tags == ('JJ', 'NN'):\n",
    "            yield tokens\n",
    "\n",
    "def ngramise(sequence):\n",
    "    '''\n",
    "    Iterate over bigrams and 1,2-skip-grams.\n",
    "    '''\n",
    "#     for unigram in sequence:\n",
    "#         yield unigram\n",
    "    for bigram in nltk.ngrams(sequence, 2):\n",
    "        yield bigram\n",
    "    for trigram in nltk.ngrams(sequence, 3):\n",
    "        yield trigram[0], trigram[2]\n",
    "    for quadgram in nltk.ngrams(sequence, 4):\n",
    "        yield quadgram[0], quadgram[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(string)\n",
    "print('\\n')\n",
    "print(nltk.pos_tag(nltk.word_tokenize(test), lang='da'))\n",
    "list(jjnn_pairs(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"text\\tidx\\tlemma\\tpunct\\tspace\\tshape\\tpos\\ttag\\tdep\\taplha\\tis_stop\".expandtabs(12))\n",
    "for token in doc:\n",
    "    print(\"{0}\\t{1}\\t{2}\\t{3}\\t{4}\\t{5}\\t{6}\\t{7}\\t{8}\\t{9}\\t{10}\".format(\n",
    "        token.text,\n",
    "        token.idx,\n",
    "        token.lemma_,\n",
    "        token.is_punct,\n",
    "        token.is_space,\n",
    "        token.shape_,\n",
    "        token.pos_,\n",
    "        token.tag_, \n",
    "        token.dep_, \n",
    "        token.is_alpha, \n",
    "        token.is_stop\n",
    "    ).expandtabs(12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [x.text for x in doc] tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for token in doc:\n",
    "#     print(token, token.lemma, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = ''\n",
    "for token in doc:\n",
    "    string = string +' '+ token.lemma_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ent in doc.ents:\n",
    "#     print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for chunk in doc.noun_chunks:\n",
    "#     print(chunk.text, chunk.root.text, chunk.root.dep_, chunk.root.head.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from spacy.lemmatizer import Lemmatizer\n",
    "# from spacy.lang.en import LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES\n",
    "\n",
    "# lemmatizer = Lemmatizer(LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES)\n",
    "# lemmas = lemmatizer(u'ducks', u'NOUN')\n",
    "# print(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Lemmy\n",
    "# https://github.com/sorenlind/lemmy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lemmy.pipe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance of Lemmy's pipeline component for spaCy\n",
    "pipe = lemmy.pipe.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the comonent to the spaCy pipeline.\n",
    "nlp.add_pipe(pipe, after='ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmas can now be accessed using the `._.lemma` attribute on the tokens\n",
    "doc = nlp(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmas can now be accessed using the `._.lemma` attribute on the tokens\n",
    "nlp(test)[7]._.lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = ''\n",
    "for token in doc:\n",
    "    string = string +' '+ token.lemma_# + [0]._.lemma\n",
    "string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = ''\n",
    "for token in doc:\n",
    "    string = string +' '+ token.lemma_\n",
    "string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _print_example(text):\n",
    "    row_format = \"{token:12}| {pos:12}| {lemma:12}\"\n",
    "    print(row_format.format(token=\"TOKEN\", pos=\"POS\", lemma=\"LEMMA\"))\n",
    "    print(\"-\"*36)\n",
    "    rows = [(t.orth_, t.pos_, t._.lemma if t._.lemma else \"None\") for t in nlp(text)]\n",
    "    for token, pos, lemma in rows:\n",
    "        print(row_format.format(token=token, pos=pos, lemma=lemma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_print_example(\"afholdt \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:aka-diagnostic]",
   "language": "python",
   "name": "conda-env-aka-diagnostic-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
