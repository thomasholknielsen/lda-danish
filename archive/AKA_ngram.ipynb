{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_get_tokens(df, col_name, n_gram=2):\n",
    "    \"\"\" Create tokens of 'col_name \"\"\"\n",
    "    def _token_creator(sentence):\n",
    "        replaced_punctation = list(map(lambda token: re.sub(r'[^\\wa-zA-Z0-9!?]+', '', token), sentence))\n",
    "        removed_punctation = list(filter(lambda token: not token.isdigit(), replaced_punctation))\n",
    "        removed_empty = list(filter(None, removed_punctation))\n",
    "        \n",
    "        replace_ = list(map(lambda token: re.sub(r'^_|(\\d)+(_$|)|_\\W|\\W_', '', token), removed_empty))\n",
    "        replace_ = list(map(lambda token: re.sub(r'^_|_$', '', token), replace_))\n",
    "        removed_empty = list(filter(None, replace_))\n",
    "        \n",
    "        removed_empty = list(set(removed_empty))\n",
    "        return removed_empty\n",
    "    \n",
    "    if n_gram == 1:\n",
    "        df['tokenized_text'] = list(map(nltk.word_tokenize, df[col_name]))\n",
    "    else:\n",
    "        df['tokenized_text'] = df[col_name].apply(lambda x: ['_'.join(ng) for ng in nltk.everygrams(nltk.word_tokenize(x), 1, n_gram)])\n",
    "    df['tokenized_text'] = list(map(_token_creator, df.tokenized_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'text':['this is a foo bar sentences and i want to ngramize it', 'Out of context, but its strange seeing Shifu and Student in one thread','Is there any way to use N-gram to check a whole document such as txt ? I am not familiar with Python so I dont know if it can open up a txt file and then use the N-gram analysis to check through ?','Her er et hus','min ledighedserklæring er netop godkendt med en dagpengesats på 607 kr om dagen. hvad er årsagen til, at jeg ikke får maks-satsen?\\n-jeg har haft fuldtidsarbejde de sidste 5 måneder med over 500 timer og en løn på cirka 24.500.'],\n",
    "    'id':[1,2,3,4,5]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_get_tokens(df, 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>tokenized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>this is a foo bar sentences and i want to ngra...</td>\n",
       "      <td>1</td>\n",
       "      <td>[i, want, bar_sentences, a, bar, it, ngramize_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Out of context, but its strange seeing Shifu a...</td>\n",
       "      <td>2</td>\n",
       "      <td>[in, Out, one, in_one, its_strange, of, but_it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Is there any way to use N-gram to check a whol...</td>\n",
       "      <td>3</td>\n",
       "      <td>[Ngram_analysis, the, any, familiar, the_Ngram...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Her er et hus</td>\n",
       "      <td>4</td>\n",
       "      <td>[Her_er, Her, hus, et, er, et_hus, er_et]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>min ledighedserklæring er netop godkendt med e...</td>\n",
       "      <td>5</td>\n",
       "      <td>[får, er, haft_fuldtidsarbejde, godkendt, er_n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  id  \\\n",
       "0  this is a foo bar sentences and i want to ngra...   1   \n",
       "1  Out of context, but its strange seeing Shifu a...   2   \n",
       "2  Is there any way to use N-gram to check a whol...   3   \n",
       "3                                      Her er et hus   4   \n",
       "4  min ledighedserklæring er netop godkendt med e...   5   \n",
       "\n",
       "                                      tokenized_text  \n",
       "0  [i, want, bar_sentences, a, bar, it, ngramize_...  \n",
       "1  [in, Out, one, in_one, its_strange, of, but_it...  \n",
       "2  [Ngram_analysis, the, any, familiar, the_Ngram...  \n",
       "3          [Her_er, Her, hus, et, er, et_hus, er_et]  \n",
       "4  [får, er, haft_fuldtidsarbejde, godkendt, er_n...  "
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min ledighedserklæring er netop godkendt med en dagpengesats på 607 kr om dagen. hvad er årsagen til, at jeg ikke får maks-satsen?\n",
      "-jeg har haft fuldtidsarbejde de sidste 5 måneder med over 500 timer og en løn på cirka 24.500.\n"
     ]
    }
   ],
   "source": [
    "print(df['text'][row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['får', 'er', 'haft_fuldtidsarbejde', 'godkendt', 'er_netop', 'fuldtidsarbejde_de', 'kr', 'at_jeg', 'årsagen_til', 'får_makssatsen', 'makssatsen', 'og_en', 'med_en', 'hvad_er', 'de_sidste', 'en_løn', 'min_ledighedserklæring', 'haft', 'dagpengesats', 'timer_og', 'måneder', 'over', 'kr_om', 'om_dagen', 'jeg', 'jeg_har', 'på_cirka', 'en_dagpengesats', 'har', 'jeg_ikke', 'løn_på', 'måneder_med', 'om', 'netop', 'timer', 'ledighedserklæring_er', 'dagen', 'netop_godkendt', '?', 'ikke', 'er_årsagen', 'har_haft', 'med_over', 'ikke_får', 'hvad', 'at', 'på', 'ledighedserklæring', 'fuldtidsarbejde', 'godkendt_med', 'dagpengesats_på', 'sidste', 'årsagen', 'løn', 'cirka', 'med', 'til', 'de', 'min', 'og', 'en']\n"
     ]
    }
   ],
   "source": [
    "print(df['tokenized_text'][row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('this', 'is')\n",
      "('is', 'a')\n",
      "('a', 'foo')\n",
      "('foo', 'bar')\n",
      "('bar', 'sentences')\n",
      "('sentences', 'and')\n",
      "('and', 'i')\n",
      "('i', 'want')\n",
      "('want', 'to')\n",
      "('to', 'ngramize')\n",
      "('ngramize', 'it')\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "map() must have at least two arguments.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-241-c978efd2cbd0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m   \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mngrams\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: map() must have at least two arguments."
     ]
    }
   ],
   "source": [
    "sentence = 'this is a foo bar sentences and i want to ngramize it'\n",
    "\n",
    "n = 2\n",
    "sixgrams = nltk.ngrams(sentence.split(), n)\n",
    "\n",
    "for grams in sixgrams:\n",
    "  print(grams)\n",
    "\n",
    "x = list(map(nltk.ngrams,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:akademikernes_diagnostic]",
   "language": "python",
   "name": "conda-env-akademikernes_diagnostic-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
